#!/usr/bin/env python3

import os
from datetime import datetime
import argparse
import random
import subprocess
import json
import re
import shutil

# ACTION IN-PLACE PROCESSING
def generate_random_key():
    """Generate a 5-character alphanumeric key with at least one digit."""
    key = ""
    digit_placed = False
    for i in range(5):
        if i == 4 and not digit_placed:
            key += str(random.randint(0, 9))
        else:
            if random.choice([True, False]):
                key += str(random.randint(0, 9))
                digit_placed = True
            else:
                key += chr(random.randint(65, 90))  # A-Z
    return key


def find_action_items(folder_path):
    """Find all action items in Markdown files."""
    action_items = []
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith(".md"):
                file_path = os.path.join(root, file)
                with open(file_path, "r") as f:
                    for line_number, line in enumerate(f, start=1):
                        if re.search(r"- \[ \]", line):
                            action_items.append((file_path, line_number, line.strip()))
    return action_items


def process_action_items(action_items):
    """Process action items, ensuring they contain a valid key."""
    processed_items = []
    for file_path, line_number, content in action_items:
        match = re.match(r"(- \[ \] *)([A-Z0-9]{5}.*)", content)
        if match:
            processed_items.append((file_path, line_number, content))
        else:
            new_key = generate_random_key()
            updated_content = re.sub(r"- \[ \]", f"- [ ] {new_key}", content, count=1)
            processed_items.append((file_path, line_number, updated_content))
    return processed_items


def write_to_output_file(processed_items, folder_path, actions_working_file):
    """Write processed action items to the output file."""
    with open(actions_working_file, "w") as f:
        for file_path, line_number, content in sorted(processed_items):
            relative_path = os.path.relpath(file_path, folder_path)
            f.write(f"{relative_path}\t{line_number}\t{content}\n")


def update_original_files(folder_path, actions_working_file):
    """Update the original files with edited content from the output file."""
    with open(actions_working_file, "r") as f:
        for line in f:
            parts = line.strip().split("\t")
            if len(parts) < 3:
                continue
            relative_path, line_number, content = parts[0], int(parts[1]), "\t".join(parts[2:])
            file_path = os.path.join(folder_path, relative_path)
            if os.path.exists(file_path):
                temp_file = f"{file_path}.tmp"
                with open(file_path, "r") as infile, open(temp_file, "w") as outfile:
                    for i, original_line in enumerate(infile, start=1):
                        if i == line_number:
                            outfile.write(content + "\n")
                        else:
                            outfile.write(original_line)
                shutil.move(temp_file, file_path)
            else:
                print(f"File not found: {file_path}")


# ACTION LOGGING AND REPORTING
def parse_markdown(file_path):
    data = {
        'Topic': None,
        'Timestamp': None,
        'Thread': None,
        'Actions': []
    }
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            if line.startswith('# Topic:'):
                data['Topic'] = line.split(':')[-1].strip()
            elif line.startswith('**Thread**:'):  # Parsing thread line
                data['Thread'] = line.split(':')[-1].strip()
            elif line.startswith('**Timestamp**:'):
                data['Timestamp'] = line.split(':')[-1].strip()
            elif line.strip().startswith('- ['):
                status = 'x' if 'x]' in line else '-'
                task = line.split(']')[-1].strip()
                # Extract only the first word of the task
                task_key = task.split()[0]  # This splits the task string into words and picks the first one
                data['Actions'].append((task_key, task.replace(task_key, ""), status))
    return data


def parse_markdown_folder(folder_path):
    actions = []
    for filename in os.listdir(folder_path):
        if filename.endswith('.md'):
            file_path = os.path.join(folder_path, filename)
            
            markdown_data = parse_markdown(file_path)

            for task_key, task, status in markdown_data['Actions']:
                current_time = datetime.now()
                timestamp_datetime = datetime.strptime(markdown_data['Timestamp'], '%Y-%m-%d-%H-%M-%S')
                days_interval = (current_time - timestamp_datetime).days

                # Regular expression to find bracketed content
                match = re.match(r'\s*\(([^)]+)\)', task)
                if match:
                    assignee = match.group(1)
                    action_item_text = re.sub(r'\s*\([^)]+\)', '', task).strip()
                else:
                    assignee = ''
                    action_item_text = task

                action = {
                        'key': task_key,
                        'filename': os.path.basename(file_path),
                        'topic': markdown_data['Topic'],
                        'timestamp': timestamp_datetime.strftime('%Y-%m-%d %H:%M:%S'),
                        'current_time': current_time.strftime('%Y-%m-%d %H:%M:%S'),
                        'thread': markdown_data.get('Thread', 'N/A'),  # Add thread to CSV
                        'action_item_text': task,
                        'assignee': assignee,
                        'text': action_item_text,
                        'status': status,
                        'days_interval': days_interval
                    }
                actions.append(action)
    return(actions)
     
      
# Function to load JSON data from a file
def load_json(filename):
    with open(filename, 'r') as file:
        return json.load(file)   
     
         
def process_markdown_folder(folder_path, new_actions_path, actions_log_path):
    actions = parse_markdown_folder(folder_path)
    
    # Load actions_log_data if the file exists, otherwise use actions_new_data
    if os.path.exists(actions_log_path):
        actions_log_data = load_json(actions_log_path)
    else:
        actions_log_data = actions

    # Create a dictionary to track records by key for actions_log_data
    actions_log_dict = {record['key']: record for record in actions_log_data}    
        
    # Process new data
    for new_record in actions:
        key = new_record['key']
        if key not in actions_log_dict:
            print("Added:")
            print(json.dumps(new_record, indent=4))
            # Rule 1: Add record if key is not present in actions_log_data
            actions_log_dict[key] = new_record
        else:
            # Rule 2: Update record if key is present and status is NOT "x"
            if actions_log_dict[key]['status'] != 'x':
                actions_log_dict[key] = new_record

    # Convert the dictionary back to a list
    updated_actions_log_data = list(actions_log_dict.values())

    # Writing the list of JSON records to a file
    with open(new_actions_path, 'w') as file:
        json.dump(actions, file, indent=4)

    # Optionally, save the updated data back to the file
    with open(actions_log_path, 'w') as file:
        json.dump(updated_actions_log_data, file, indent=4)


def load_action_log(actions_log_path):
    # Load actions_log_data if the file exists, otherwise use actions_new_data
    if os.path.exists(actions_log_path):
        actions = load_json(actions_log_path)
        return(actions)
    else:
        print("No actions log path found.")
        return

def filter_data(data, filters):
    filtered = []
    for item in data:
        match = True
        for key, value in filters.items():
            if key in item and value.lower() not in str(item[key]).lower():
                match = False
                break
        if match:
            filtered.append(item)
    return filtered


def query_actions(actions):
    dynamic_parser = argparse.ArgumentParser(description="Query actions with dynamic filters")
    keys = actions[0].keys()
    # Add arguments dynamically based on the keys
    for key in keys:
        dynamic_parser.add_argument(f'--{key}', type=str, help=f'Filter by {key}')
    dynamic_args, unknown = dynamic_parser.parse_known_args()
    filters = {key: value for key, value in vars(dynamic_args).items() if value is not None}
    filtered_results = filter_data(actions, filters)
    return(filtered_results)


def report_actions(actions):
        sorted_results = sorted(actions, key=lambda x: (x['assignee'], x['status'], x['days_interval']))
        # Get the terminal width
        terminal_width = shutil.get_terminal_size().columns
        # Define the minimum column widths
        min_col_widths = {
            "Thread": 15,
            "Assignee": 15,
            "Status": 10,
            "Days": 10,
            "Action Item": 100
        }

        # Calculate total width of fixed columns
        fixed_width = sum(min_col_widths.values()) + len(min_col_widths) - 1  # Account for spacing
        print(fixed_width)

        # Calculate width for action_item column
        action_item_width = max(0, terminal_width - fixed_width + 100)
        print(action_item_width)
        headers = ["Thread", "Assignee", "Status", "Days", "Action Item"]
                
        # Print the header row
        print(f"{headers[0]:<{min_col_widths['Thread']}} {headers[1]:<{min_col_widths['Assignee']}} {headers[2]:<{min_col_widths['Status']}} {headers[3]:<{min_col_widths['Days']}} {headers[4]:<{action_item_width}}")
        print("-" * terminal_width)
                    
        # Print the sorted data rows
        for record in sorted_results:
            print(f"{record['thread']:<{min_col_widths['Thread']}} {record['assignee']:<{min_col_widths['Assignee']}} {record['status']:<{min_col_widths['Status']}} {record['days_interval']:<{min_col_widths['Days']}} {record['text'][:action_item_width]:<{action_item_width}}")


# MAIN FUNCTION
def main():
    parser = argparse.ArgumentParser(description="""Process and query actions from markdown notes. 
                                     By default this command opens a list of all open action items found in all your notes in the nano text editor for modification.
                                     Once you are done modifying, save and close the file and all of the notes will have your changes inserted in place.""")
    parser.add_argument("--noninteractive", action="store_true", help="Update notes with new information and update the action log without opening the actions working file in nano.")
    parser.add_argument("--query", action='store_true', help="Query action items log. Returns full json records.  Example: actions --query --thread personal --status x")
    parser.add_argument("--report", action='store_true', help="Query action items log. Returns a table. Example: actions --report --thread personal --status x")
    parser.add_argument("--delete", action='store_true', help="Open the action log to manually delete actions.")

    args, unknown = parser.parse_known_args()

    home_path = os.path.expanduser('~')
    folder_path = os.path.join(home_path, '.adulting', 'notes')
    actions_working_file = os.path.join(folder_path, ".action_items.md")

    actions_new_file = os.path.join(home_path, '.adulting', 'actions_current.json')
    actions_log_file = os.path.join(home_path, '.adulting', 'actions_log.json')

    if not os.path.exists(folder_path):
        print(f"Notes directory does not exist: {folder_path}")
        return

    if os.path.exists(actions_working_file):
        os.remove(actions_working_file)

    # Update actions in markdown to ensure they have proper keys
    action_items = find_action_items(folder_path)
    processed_items = process_action_items(action_items)
    write_to_output_file(processed_items, folder_path, actions_working_file)
    update_original_files(folder_path, actions_working_file)
    process_markdown_folder(folder_path, actions_new_file, actions_log_file)

    if args.query:
        actions = load_action_log(actions_log_file)
        filtered_results = query_actions(actions)
        print(json.dumps(filtered_results, indent=4))
    elif args.report:
        actions = load_action_log(actions_log_file)
        filtered_results = query_actions(actions)
        report_actions(filtered_results)
    elif args.delete:
        os.system(f"nano {actions_log_file}")
    elif args.noninteractive:
        with open(actions_working_file, "r") as f:
            print(f.read())
    else:
        os.system(f"nano {actions_working_file}")
        update_original_files(folder_path, actions_working_file)
        process_markdown_folder(folder_path, actions_new_file, actions_log_file)
    
    os.remove(actions_working_file)
    print('Actions log updated.')



if __name__ == "__main__":
    main()
